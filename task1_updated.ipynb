{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc8f9bf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This file is concerned with the first asssignment in the ST 443 group project. The task is to classify the observation to one of the eight vegetation classes based on the reflectance values for each pixel in the i-th wavelenght band, i $ \\ \\in \\ \\{1, 2, \\ldots, 218\\}$. We will start with T1.1, which is concerned with data visualization and understanding the distribution of the features, and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2e4f6",
   "metadata": {},
   "source": [
    "## Data Preparation and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904bef3",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports from the standard library\n",
    "import sys\n",
    "\n",
    "#Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff222d",
   "metadata": {},
   "source": [
    "### Read the zipped csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the path with the absolute path of your file\n",
    "data1=pd.read_csv('data-1.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17733852",
   "metadata": {},
   "source": [
    "## Task 1.1: Data Visualization and summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25b64e",
   "metadata": {},
   "source": [
    "Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataframe\n",
    "print(f\"\\n Number of Rows: {data1.shape[0]} \\n Number of Columns: {data1.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datatypes of all the columns in the dataset\n",
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information about the dataset\n",
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the first 5 entries\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022707b7",
   "metadata": {},
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a443885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics of the float columns\n",
    "data1.select_dtypes(include = \"float64\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2e1e7",
   "metadata": {},
   "source": [
    "Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing values\n",
    "missing_features = data1.select_dtypes(include = \"float64\").isna().sum().sum()\n",
    "missing_target = data1['land_type'].isna().sum()\n",
    "print(f\"Total missing values in the feature space: {missing_features}\")\n",
    "print(f\"Total missing target values: {missing_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42d291",
   "metadata": {},
   "source": [
    "Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicates check\n",
    "data1[data1.duplicated() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c0823",
   "metadata": {},
   "source": [
    "No duplicates in this dataset. We can check for invalid values. Reflectance values should be in $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54156a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [col for col in data1.columns if col.startswith(\"B\")]\n",
    "print(\"Negative reflectance values:\", (data1[bands] < 0).sum().sum())\n",
    "print(\"Reflectance values > 1:\", (data1[bands] > 1).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ce69d",
   "metadata": {},
   "source": [
    "There are invalid values in the dataset, so we should clip them, such that the minimum value is 0, and the maximum value is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b078d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clip the extreme values\n",
    "data1[bands] = data1[bands].clip(lower=0, upper=1)\n",
    "print(\"After clipping:\")\n",
    "print(\"Min reflectance:\", data1[bands].min().min())\n",
    "print(\"Max reflectance:\", data1[bands].max().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d695c087",
   "metadata": {},
   "source": [
    "Function to check for outliers based on the Z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e01ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_check(df, cols, z):\n",
    "    z_score = np.abs((df[cols] - df[cols].mean()) / df[cols].std())\n",
    "    outliers_df = df[np.any(z_score > z, axis = 1)]\n",
    "    return outliers_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2acc131",
   "metadata": {},
   "source": [
    "We can check for values of 3 standard deviations away from mean to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_number = outlier_check(data1, bands, 3)\n",
    "percentage_of_outliers = outliers_number/data1.shape[0] * 100\n",
    "print(f\"Percentage of outliers: {percentage_of_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23170331",
   "metadata": {},
   "source": [
    "Very few outliers, and we were very severe in flagging them. Normally, we would not expect features to follow a normal distribution, so there would be less outliers. No reason to drop them. We should check for imbalance by looking at the distribution of the vegetation classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245723b",
   "metadata": {},
   "source": [
    "Distribution of vegetation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075db7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check distribution of vegetation classes to indentify potential imbalance(may affect classification later)\n",
    "class_counts = data1[\"land_type\"].value_counts()\n",
    "class_percent = data1[\"land_type\"].value_counts(normalize=True) * 100\n",
    "\n",
    "balance_df = pd.DataFrame({\n",
    "    \"Count\": class_counts,\n",
    "    \"Percentage\": class_percent.round(2)\n",
    "}).reset_index().rename(columns={\"index\": \"Land Type\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=\"land_type\", data=data1)\n",
    "plt.title(\"Distribution of Vegetation Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e40e19",
   "metadata": {},
   "source": [
    "We can observe that the alpine meadopw is the most frequent vegetation class, with around a quarter of the observations classified in this class. Also, valley floor and alpine tundra are also quite prevalent, and about 58% of all the observations are classified into one of this vegetation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial distribution of land types (confirms figure 1 in the project description)\n",
    "sample = data1.sample(10000, random_state=0)\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=\"p_x\", y=\"p_y\", hue=\"land_type\", data=sample, s=8, linewidth=0)\n",
    "plt.title(\"Spatial Distribution of Land Types\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713104cd",
   "metadata": {},
   "source": [
    "We should check if classes are separable in the spectral space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if classes are separable in spectral space\n",
    "mean_spectra = data1.groupby(\"land_type\")[bands].mean().T\n",
    "mean_spectra\n",
    "\n",
    "# Lines that differ strongly → those classes are spectrally separable → classification should work well.\n",
    "# Overlapping lines → those classes are spectrally similar → may need nonlinear models (e.g. GBDT / SVM).\n",
    "# Smoothness across bands → confirms that adjacent bands are highly correlated. Motivates PCA or regularisation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spectra.plot(figsize=(10,5))\n",
    "plt.xlabel(\"Spectral band index (~420–2450 nm)\")\n",
    "plt.ylabel(\"Mean surface reflectance\")\n",
    "plt.title(\"Mean Spectral Signature by Land Type\")\n",
    "plt.legend(bbox_to_anchor=(1.05,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3ab38",
   "metadata": {},
   "source": [
    "Note that for Lines that differ strongly it means those classes are spectrally separable, so classification should work well. For the Overlapping lines the classes are spectrally similar, so may need nonlinear models (e.g. GBDT / SVM).\n",
    "There is smoothness across bands, which confirms that adjacent bands are highly correlated. Motivates PCA or regularisation later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eebf24",
   "metadata": {},
   "source": [
    "Inspect the correlations across the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation for the first 50 bands\n",
    "corr50 = data1[bands[:50]].corr()\n",
    "corr50.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation heatmap for the first 50 bands\n",
    "sns.heatmap(corr50, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation among first 50 bands\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation among all the bands\n",
    "corrfull = data1[bands].corr()\n",
    "corrfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16612b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation heatmap for all bands\n",
    "sns.heatmap(corrfull, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation among all bands\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data1[\"Band_1\"], data1[\"Band_2\"])\n",
    "plt.xlabel(\"Band1\")\n",
    "plt.ylabel(\"Band2\")\n",
    "plt.title(\"Scatterplot of Band 1 and Band 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((data1[\"Band_1\"] - data1[\"Band_1\"].mean())/data1[\"Band_1\"].std(), (data1[\"Band_214\"] - data1[\"Band_214\"].mean())/data1[\"Band_214\"].std())\n",
    "plt.xlabel(\"Band 1\")\n",
    "plt.ylabel(\"Band 214\")\n",
    "plt.title(\"Scatterplot of Z-Scored Band 1 and Band 214\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e712344",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((data1[\"Band_217\"] - data1[\"Band_217\"].mean())/data1[\"Band_217\"].std(), (data1[\"Band_214\"] - data1[\"Band_214\"].mean())/data1[\"Band_214\"].std())\n",
    "plt.xlabel(\"Band 217\")\n",
    "plt.ylabel(\"Band 214\")\n",
    "plt.title(\"Scatterplot of Z Scored Band 217 and Band 214\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bbca0",
   "metadata": {},
   "source": [
    "We can observe that the correlation is larger among neighboring features. For example the average correlation is much larger when only the first 50 bands are considered than when all the bands are considered. Indeed, we can observe that correlation even becomes negative for some distant bands. We have defined distant bands in a very rough way, as the difference between distance labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a41402",
   "metadata": {},
   "source": [
    "We should also look at the conditional distributions of the bands to better understand the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data1[data1[\"land_type\"] == \"alpine meadow\"][\"Band_1\"], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data1[\"Band_1\"], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81112d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data1[data1[\"land_type\"] == \"snow / ice\"][\"Band_1\"], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a40c4",
   "metadata": {},
   "source": [
    "We can observe heterogeneous conditional distributions. In particular, Band 1 values are larger for the land type snow/ice, and many of the values achieve the maximumm threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb99e8",
   "metadata": {},
   "source": [
    "Before fitting the models, we will do two more tasks: Clustering and PCA. Clustering will helps us visualize the features more, and PCA will be able to help us for dimensionality reduction. Due to the large number of features, it might be helpful to do Clustering on the first principal components. We will apply PCA to the training set, and then clustering. PCA will help evaluate whether we can reduce dimensionality, while clustering will aid us in visualizing the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d690e",
   "metadata": {},
   "source": [
    "Find the bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1812f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bands = data1[bands]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f864b1d",
   "metadata": {},
   "source": [
    "Perform PCA using 4 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ebb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = StandardScaler().fit_transform(X_bands)\n",
    "pca  = PCA(n_components=4).fit(X_bands)\n",
    "print(f\"\\n Percentage of Variance explained by the first four principal components: {np.round(sum(pca.explained_variance_ratio_) * 100, 3)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0693b",
   "metadata": {},
   "source": [
    "Percentage of explained Variance by each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = pd.DataFrame({\"Component\": [1, 2, 3, 4], \"Percentage of Explained Variance\": np.round(pca.explained_variance_ratio_ * 100, 3)}).set_index(\"Component\")\n",
    "principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e122bf",
   "metadata": {},
   "source": [
    "PCA singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_values = pd.DataFrame({\"Component\": [1, 2, 3, 4], \"Singular Value\": pca.singular_values_}).set_index(\"Component\")\n",
    "singular_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c521c6b",
   "metadata": {},
   "source": [
    "PCA eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors = pd.DataFrame(pca.components_)\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac493c1",
   "metadata": {},
   "source": [
    "We can observe that only the first three principal components capture 99% of the variance in the feature space, so we can retain only the frist 3 principal components. This suggests that dimensionality reduction is achievable on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = PCA(n_components=3).fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be654932",
   "metadata": {},
   "source": [
    "We will now perform clustering on the reduced dataset using K-Means. It is better to apply K-Means on the PCA transformed dataset for stability. For visualization purposes, we will perform clustering on the first two principal components only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = Z[:, 0]\n",
    "pc2 = Z[:, 1]\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "cluster = kmeans.fit_predict(Z[:, 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da999c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = plt.scatter(pc1, pc2, c = cluster)\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1],\n",
    "            marker=\"o\", s=200, edgecolor=\"k\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"K-Means Clusters in PCA Space\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c6669",
   "metadata": {},
   "source": [
    "The K-Means does not aid us too much in visualizing the features when we use the first 2 PCs. Clusters seem arbitrarily separated, and, indeed, the scatteplot does not suggest clustering in the PCA space. As such, we will only use PCA for dimensionality reduction and model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af946a0",
   "metadata": {},
   "source": [
    "## Task 1.2 - Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d70d7",
   "metadata": {},
   "source": [
    "In this part of the project, we will try to predict the Vegetation type based on the reflectance values of the pixels. This is a MultiClass classification problem, and we will use several types of classifiers for this task. To encode the variables from categorical to numerical, we will use Scikit Learn's Label Encoder. The methodology will be the following: we will fit the models and report the cross-validation errors on the training set to adjust the hyperparamters. After finding the best model, we will fit it to the whole training data, and report its performance on the test set. The test set will be used only to report the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c612726",
   "metadata": {},
   "source": [
    "### Define functions to evaluate performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1f9d4",
   "metadata": {},
   "source": [
    "Create function to report cross-validation performance - This will help us tweak the hyperparameters in case it's necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cv(estimator, X, y, scoring_list, model_name = \"model\"):\n",
    "    \n",
    "    #Define StratifiedKFold splitter. This makes sure than no split is more imbalanced than the original data\n",
    "    cv_splitter = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "    #Calculate the CV score for each metric\n",
    "    cv_results = cross_validate(\n",
    "        estimator, \n",
    "        X, \n",
    "        y, \n",
    "        scoring = scoring_list, \n",
    "        cv = cv_splitter, \n",
    "        return_train_score=False\n",
    "    )\n",
    "\n",
    "    #Create a dataframe with all the performance metrics using cv\n",
    "    row = {\"model\": model_name}\n",
    "    for k, v in cv_results.items():\n",
    "        if k.startswith(\"test_\"):\n",
    "            metric = k.replace(\"test_\", \"\")\n",
    "            row[f\"{metric}\"] = np.mean(v)\n",
    "        \n",
    "    return pd.DataFrame([row]).set_index(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c1b1a",
   "metadata": {},
   "source": [
    "Function to evaluate the performance on the test set, and return the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c320971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evaluate(estimator, X_tr: pd.DataFrame, X_ts: pd.DataFrame, y_tr: pd.DataFrame, y_ts, model_name: str):\n",
    "\n",
    "    #Fit the estimator on the training data\n",
    "    classifier = estimator.fit(X_tr, y_tr)\n",
    "\n",
    "    #Find the predictions and probabilities\n",
    "    predictions = classifier.predict(X_ts)\n",
    "    predictions_prob = classifier.predict_proba(X_ts)\n",
    "\n",
    "    #Calculate the error metrics using predictions\n",
    "    acc = accuracy_score(y_ts, predictions)\n",
    "    balanced_acc = balanced_accuracy_score(y_ts, predictions)\n",
    "    f1 = f1_score(y_ts, predictions, average=\"macro\")\n",
    "\n",
    "    #Calculate auroc using the probabilities\n",
    "    auc = roc_auc_score(y_ts, predictions_prob, average = \"macro\", multi_class=\"ovr\")\n",
    "\n",
    "    return pd.DataFrame({\"model\": [model_name], \"Accuracy\": [acc], \"Bal. Acc\": [balanced_acc], \"AUC\": [auc], \"F1\": [f1]}), predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653c7ca",
   "metadata": {},
   "source": [
    "Generalization of the previous function to test the performance of multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a47447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_predict_evaluate(models:list[tuple], X_tr: pd.DataFrame, X_ts: pd.DataFrame, y_tr, y_ts):\n",
    "\n",
    "    #Create a list of dataframes to hold the performances of each model\n",
    "    metrics_list = []\n",
    "\n",
    "    #Create a list of dataframes to hold the predictions from each model\n",
    "    predictions_list = []\n",
    "\n",
    "    #Iterate through each model\n",
    "    for model, estimator in models:\n",
    "        \n",
    "        #report the performance and predicitions of the model\n",
    "        metric, preds = predict_evaluate(estimator, X_tr, X_ts, y_tr, y_ts, model)\n",
    "\n",
    "        #Append the performance to the metrics list\n",
    "        metrics_list.append(metric)\n",
    "\n",
    "        #Create dataframe to hold the predictions\n",
    "        df_model = pd.DataFrame({f\"{model}\": preds})\n",
    "\n",
    "        #Append the dataframe to the  predictions_list\n",
    "        predictions_list.append(df_model)\n",
    "\n",
    "    #Return the concatenated dataframes\n",
    "    return pd.concat(metrics_list, ignore_index = True), pd.concat(predictions_list, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8035d8",
   "metadata": {},
   "source": [
    "Function to plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae79a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, normalize, model_name):\n",
    "\n",
    "    #Create figure\n",
    "    fig = plt.figure(figsize=(12, 12), dpi=100)  # FORCE a large, readable figure\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    #Create Confusion Matrix display\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        display_labels=class_names,\n",
    "        normalize=normalize,\n",
    "        cmap=\"Blues\",\n",
    "        colorbar=True,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\", fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fddae",
   "metadata": {},
   "source": [
    "### Preparations of the features and the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac81ed4e",
   "metadata": {},
   "source": [
    "Find the features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372647ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data1.drop(columns=[\"land_type\",\"rgb_hex\", \"overlay_hex\"])\n",
    "target  = data1[\"land_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375f2b4",
   "metadata": {},
   "source": [
    "Split into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General train/test split, for models\\svm, knn\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, \n",
    "    target, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True,\n",
    "    stratify=target #This ensures the train and test set follow the same distribution as the original dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51171949",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_1 = X_train.sample(30000, random_state=42)\n",
    "y1_1 = y_train.loc[data1_1.index]\n",
    "\n",
    "X_train_1 = data1_1\n",
    "y_train_1 = y1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7929ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Number of observations in the training set: {X_train.shape[0]}\", f\"\\n Number of observations in the test set: {X_test.shape[0]}\")\n",
    "\n",
    "print(f\"\\n Number of observations in the training set (sampled data): {X_train_1.shape[0]}\", f\"\\n Number of observations in the test set: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d838b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47249863",
   "metadata": {},
   "source": [
    "Check wehether there is any overlap between the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(X_train.index).isdisjoint(set(X_test.index))\n",
    "assert set(X_train_1.index).isdisjoint(set(X_test.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ed78c",
   "metadata": {},
   "source": [
    "No overlap between the training set and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b8b877",
   "metadata": {},
   "source": [
    "Use LabelEncoder to transform categorical targets into integer targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_1 = LabelEncoder()\n",
    "y_train_encoded_1 = le_1.fit_transform(y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9729cd9",
   "metadata": {},
   "source": [
    "Training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739daa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ce951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cde783",
   "metadata": {},
   "source": [
    "Test target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f369173",
   "metadata": {},
   "source": [
    "Classes as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3564d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train_encoded_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1f109",
   "metadata": {},
   "source": [
    "### Create pipelines to fit the models. One pipeline will fit on the raw data, and the other one will fit on the data after PCA with 10 components is applied for each model considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logistic = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()),\n",
    "     (\"logit_classifier\", LogisticRegression(\n",
    "         solver= \"lbfgs\", \n",
    "         max_iter=2000                 # increase to avoid convergence warning\n",
    "     ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_logistic_pca = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()),\n",
    "     (\"pca\", PCA(n_components=10)),\n",
    "     (\"logistic_classifier\", LogisticRegression(\n",
    "         solver = \"lbfgs\", #saga is fater for larger datasets\n",
    "         max_iter=2000                 # increase to avoid convergence warning\n",
    "     ))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define LDA pipelines: one raw (full data), one with PCA10\n",
    "lda_raw = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "lda_pca10 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=10, random_state=42)),\n",
    "    (\"clf\", LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "\n",
    "# Define QDA pipelines: one raw (full data), one with PCA10\n",
    "qda_raw = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", QuadraticDiscriminantAnalysis(reg_param=0.01))\n",
    "])\n",
    "\n",
    "qda_pca10 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=10, random_state=42)),\n",
    "    (\"clf\", QuadraticDiscriminantAnalysis(reg_param=0.01))\n",
    "])\n",
    "\n",
    "# Define Random Forest pipelines: one raw (full data), one with PCA10\n",
    "rf_raw = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        criterion='gini',\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features='sqrt'))\n",
    "])\n",
    "\n",
    "rf_pca10 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=10, random_state=42)),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        criterion='gini',\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        max_features='sqrt'))\n",
    "])\n",
    "\n",
    "# Define GBDT pipelines: one raw (full data), one with PCA10\n",
    "gbdt_raw = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.15,\n",
    "        max_depth=2,\n",
    "        subsample=0.7,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=42))\n",
    "])\n",
    "\n",
    "gbdt_pca10 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "     (\"pca\", PCA(n_components=10, random_state=42)),\n",
    "    (\"clf\", GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.15,\n",
    "        max_depth=2,\n",
    "        subsample=0.7,\n",
    "        max_features=\"sqrt\",\n",
    "        random_state=42))\n",
    "])\n",
    "\n",
    "# ==============\n",
    "# KNN pipelines\n",
    "# ==============\n",
    "\n",
    "knn_raw = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", KNeighborsClassifier(\n",
    "        n_neighbors=7,\n",
    "        weights=\"distance\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "knn_pca10 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=10, random_state=42)),\n",
    "    (\"clf\", KNeighborsClassifier(\n",
    "        n_neighbors=7,\n",
    "        weights=\"distance\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ==============\n",
    "# SVM pipelines\n",
    "# ==============\n",
    "\n",
    "svm_raw = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=20,\n",
    "        gamma=\"scale\",\n",
    "        probability=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "svm_pca10 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\", PCA(n_components=10, random_state=42)),\n",
    "    (\"clf\", SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=20,\n",
    "        gamma=\"scale\",\n",
    "        probability=True\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f935d",
   "metadata": {},
   "source": [
    "### Cross Validation performance on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21253d62",
   "metadata": {},
   "source": [
    "Create a list of metrics that we will use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240cf24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_list = {\"Accuracy\": \"accuracy\", \"Bal. Acc\": \"balanced_accuracy\", \"AUC\": \"roc_auc_ovr\", \"F1\":\"f1_macro\"}\n",
    "#Macro means that we are taking the average over the number of classes - sensitive to class imbalance.\n",
    "#ovr means that we treat one class as positive, the rest as negative for all the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d04fdd",
   "metadata": {},
   "source": [
    "Corss validation performance of logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_cv = evaluate_cv(pipe_logistic, X_train, y_train_encoded, scoring_list=scoring_list, model_name=\"Logistic\") # Approx 8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf693b",
   "metadata": {},
   "source": [
    "Cross validation performance of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc215ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cv_raw = evaluate_cv(lda_raw, X_train, y_train, scoring_list=scoring_list, model_name=\"LDA Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64536ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_cv_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28090e90",
   "metadata": {},
   "source": [
    "Cross validation performance of QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_cv_raw = evaluate_cv(qda_raw, X_train, y_train, scoring_list=scoring_list, model_name=\"QDA Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_cv_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4799e4",
   "metadata": {},
   "source": [
    "Cross validation performance of Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv_raw = evaluate_cv(rf_raw, X_train, y_train, scoring_list=scoring_list, model_name=\"RF Raw\") # Approx 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a5d7d",
   "metadata": {},
   "source": [
    "Cross validation performance of GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b5d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_cv_raw = evaluate_cv(gbdt_raw, X_train, y_train, scoring_list=scoring_list, model_name=\"GBDT Raw\") # Approx 11 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ae713",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt_cv_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8833bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_raw=evaluate_cv(knn_raw, X_train_1, y_train_1, scoring_list=scoring_list, model_name=\"KNN RAW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bdec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329dc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cv_raw=evaluate_cv(svm_raw, X_train_1, y_train_1, scoring_list=scoring_list, model_name=\"SVM RAW\") # Approx 2 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5501673",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_cv_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5164193",
   "metadata": {},
   "source": [
    "## Task 1.3 Model Evaluation on the test set and performance visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c6302",
   "metadata": {},
   "source": [
    "In this part of the project, we will report the performances of each classifier on the test set, which is composed of 20% of the observations. We will also visualize the confusion matrix of each model, as it provides an intuitive visualization of the performance of each classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d77e97",
   "metadata": {},
   "source": [
    "Create a list of models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of models and model names\n",
    "models_list_raw = [(\"Logistic\", pipe_logistic), \n",
    "                   (\"LDA\", lda_raw),\n",
    "                   (\"QDA\", qda_raw),\n",
    "                   (\"RF\", rf_raw),\n",
    "                   (\"GBDT\", gbdt_raw)]\n",
    "\n",
    "models_list_pca = [(\"Logistic_PCA10\", pipe_logistic_pca),\n",
    "                   (\"LDA_PCA10\", lda_pca10),\n",
    "                   (\"QDA_PCA10\", qda_pca10),\n",
    "                   (\"RF_PCA10\", rf_pca10),\n",
    "                   (\"GBDT_PCA10\", gbdt_pca10)]\n",
    "\n",
    "#--------------------------------------------->>\n",
    "# USING THE SAMPLED DATA\n",
    "#--------------------------------------------->>\n",
    "\n",
    "models_list_raw_1 = [\n",
    "    (\"KNN\", knn_raw),\n",
    "    (\"SVM\", svm_raw)\n",
    "]\n",
    "\n",
    "models_list_pca_1 = [\n",
    "    (\"KNN_PCA10\", knn_pca10),\n",
    "    (\"SVM_PCA10\", svm_pca10)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b66a5",
   "metadata": {},
   "source": [
    "Find the performance and predictions of each model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb11a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predictions and the performance of each model, both the raw and the PCA\n",
    "# Approx 11 mins\n",
    "df_performances_raw, df_predictions_raw = multiple_predict_evaluate(models_list_raw, X_train, X_test, y_train_encoded, y_test_encoded)\n",
    "df_performances_pca, df_predictions_pca = multiple_predict_evaluate(models_list_pca, X_train, X_test, y_train_encoded, y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eeac8b",
   "metadata": {},
   "source": [
    "Performances of each model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe69ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATELY FOR KNN AND SVM USING THE SUBSAMPLED DATA\n",
    "\n",
    "# Find the predictions and the performance of each model, both the raw and the PCA\n",
    "# Though we has to train on subset, we can stull test on original 20% test set to ensure comparability acorss all models\n",
    "df_performances_raw_1, df_predictions_raw_1 = multiple_predict_evaluate(models_list_raw_1, X_train_1, X_test, y_train_encoded_1, y_test_encoded)\n",
    "df_performances_pca_1, df_predictions_pca_1 = multiple_predict_evaluate(models_list_pca_1, X_train_1, X_test, y_train_encoded_1, y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a47a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performances_raw = df_performances_raw.set_index(\"model\")\n",
    "df_performances_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATELY FOR KNN AND SVM USING THE SUBSAMPLED DATA\n",
    "df_performances_raw_1= df_performances_raw_1.set_index(\"model\")\n",
    "df_performances_raw_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08339548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performances_pca = df_performances_pca.set_index(\"model\")\n",
    "df_performances_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ba35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATELY FOR KNN AND SVM USING THE SUBSAMPLED DATA\n",
    "df_performances_pca_1 = df_performances_pca_1.set_index(\"model\")\n",
    "df_performances_pca_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc261b",
   "metadata": {},
   "source": [
    "Combined test performance for easy reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98ed2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_raw = pd.concat([df_performances_raw, df_performances_raw_1], axis=0)\n",
    "df_all_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aed643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pca = pd.concat([df_performances_pca, df_performances_pca_1], axis=0)\n",
    "df_all_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f36f4e",
   "metadata": {},
   "source": [
    "Predictions of each model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47428753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd89cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_raw_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_pca_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290261d",
   "metadata": {},
   "source": [
    "Predictions mapped to original land_types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be823397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_original_raw = df_predictions_raw.apply(le.inverse_transform)\n",
    "df_pred_original_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d72898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATELY FOR KNN AND SVM USING THE SUBSAMPLED DATA\n",
    "df_pred_original_raw_1 = df_predictions_raw_1.apply(le_1.inverse_transform)\n",
    "df_pred_original_raw_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55421c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_original_pca = df_predictions_pca.apply(le.inverse_transform)\n",
    "df_pred_original_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eefc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEPARATELY FOR KNN AND SVM USING THE SUBSAMPLED DATA\n",
    "df_pred_original_pca_1 = df_predictions_pca_1.apply(le_1.inverse_transform)\n",
    "df_pred_original_pca_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b6b48",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e84721",
   "metadata": {},
   "source": [
    "Confusion matrix for Logitstic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw logistic confusion matrix\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_raw[\"Logistic\"], le.classes_, \"true\", \"Logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea57c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA logistic confusion matrix\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_pca[\"Logistic_PCA10\"], le.classes_, \"true\", \"Logistic PCA 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14699c92",
   "metadata": {},
   "source": [
    "Confusion matrix for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc9adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_encoded, df_predictions_raw[\"LDA\"], le.classes_, \"true\", \"LDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ed66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_encoded, df_predictions_pca[\"LDA_PCA10\"], le.classes_, \"true\", \"LDA PCA10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f87e1",
   "metadata": {},
   "source": [
    "Confusion matrix for QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663556c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_encoded, df_predictions_raw[\"QDA\"], le.classes_, \"true\", \"QDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ae379",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test_encoded, df_predictions_pca[\"QDA_PCA10\"], le.classes_, \"true\", \"QDA PCA 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea97059",
   "metadata": {},
   "source": [
    "Confusion Matrix for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42351809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw Random Forest confusion matrix\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_raw[\"RF\"], le.classes_, \"true\", \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec537b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA Random Forest confusion matrix\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_pca[\"RF_PCA10\"], le.classes_, \"true\", \"Random Forest PCA 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce7665",
   "metadata": {},
   "source": [
    "Confusion Matrix for GBDT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a661b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw GBDT confusion matrix\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_raw[\"GBDT\"], le.classes_, \"true\", \"GBDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA GBDT confusion matrix\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_pca[\"GBDT_PCA10\"], le.classes_, \"true\", \"GBDT PCA 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_raw_1[\"KNN\"], le_1.classes_, \"true\", \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d2f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN PCA10\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_pca_1[\"KNN_PCA10\"], le_1.classes_, \"true\", \"KNN PCA 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dad5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM \n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_raw_1[\"SVM\"], le_1.classes_, \"true\", \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM PCA10\n",
    "plot_confusion_matrix(y_test_encoded, df_predictions_pca_1[\"SVM_PCA10\"], le_1.classes_, \"true\", \"SVM PCA 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ef868",
   "metadata": {},
   "source": [
    "#### Some observations worth noting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8033e8",
   "metadata": {},
   "source": [
    "We can see that the classifiers have very high performance on this dataset, despite its large size. One possible reasons is due to the fact that in image recognition the signal to noise ratio is very high, as compared to, for example, financial time series, where the signal to noise ratio is low. Also, LDA and QDA have modest performances on this dataset, compared to the other models. This suggests that the generative approach to classifier is not necessarily suitable for this dataset, especially assuming that the features follow a conditional Multivariate Normal Distribution. QDA does improved the modelling a bit from LDA but not enough to boost our prediction confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b6ab4",
   "metadata": {},
   "source": [
    "We will create the function my_predict that reads a test file from the current directory, and predicts the land_types for each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded842cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mypredict():\n",
    "\n",
    "    #Define the model\n",
    "    model_classifier = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()),\n",
    "     (\"pca\", PCA(n_components=10)),\n",
    "     (\"logistic_classifier\", LogisticRegression(\n",
    "         solver = \"lbfgs\", #saga is fater for larger datasets\n",
    "         max_iter=2000                 # increase to avoid convergence warning\n",
    "     ))\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    #Use the whole training data to fit the model\n",
    "    X = data1.drop(columns=[\"land_type\",\"rgb_hex\", \"overlay_hex\"])\n",
    "    y = data1[\"land_type\"]\n",
    "    lbl = LabelEncoder()\n",
    "    y_encoded = lbl.fit_transform(y)\n",
    "    model_classifier.fit(X, y)\n",
    "\n",
    "    #Read the test data\n",
    "    test_data1  = pd.read_csv(\"test.csv.gz\")\n",
    "    X_test_data1 = test_data1.drop(columns = [\"land_type\", \"rgb_hex\", \"overlay_hex\"])\n",
    "    y_test_data1 = test_data1[\"land_type\"]\n",
    "    y_test_data1_encoded = lbl.transform(y_test_data1)\n",
    "\n",
    "    #Find the predictions\n",
    "    test_data1_predictions = model_classifier.predict(X_test_data1)\n",
    "\n",
    "    #Convert into dataframe\n",
    "    df_data1_predictions = pd.DataFrame({\"land_type\": test_data1_predictions})\n",
    "\n",
    "    #Map back to original classes\n",
    "    df_data1_pred_original = df_data1_predictions.apply(lbl.inverse_transform)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d0961",
   "metadata": {},
   "source": [
    "## Task 1.4 Classifying glacier ice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a7a35",
   "metadata": {},
   "source": [
    "In this part of the project, we will solve a binary classification problem. Specifically, we will frame glacier detection as a binary task with glacier ice as the positive class, and the other land types as negative. Based on the performances on the multiclass classification problem, we will choose three models: Logistic Regression fitted on the first 10 principal components, Support Vector Machines using the raw version, and Random Forest classifier on the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4be1a6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
